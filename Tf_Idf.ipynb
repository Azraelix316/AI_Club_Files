{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#DATA\n",
        "\n",
        "First, we need to import the actual data. All of this is pretty simple and easily found online for sources, so we won't explain this too much. It's just taking the database from the online\n",
        "\n",
        "The import uses wget to get the file online that we uploaded, the Persuade 2.0 Corpus that has over 25,000 student essays that were ranked on a scale of 1-6."
      ],
      "metadata": {
        "id": "Gp_p9t47PLID"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "!wget https://raw.githubusercontent.com/Azraelix316/AI_Club_Files/main/persuade_2.0_human_scores_demo_id_github.csv -O persuade_corpus_2.0_train.csv\n",
        "# Step 1: Load the training and testing datasets\n",
        "# Use the 'sep' parameter to specify the delimiter and handle quoting\n",
        "train_data = pd.read_csv(\n",
        "    \"persuade_corpus_2.0_train.csv\",\n",
        "    on_bad_lines=\"skip\",              # Skip rows with errors\n",
        "    quoting=csv.QUOTE_MINIMAL,        # Use minimal quoting (only when necessary)\n",
        "    quotechar='\"',                    # Handle quotes correctly (even though none expected)\n",
        "    engine='python',                  # Use the Python engine for better error handling\n",
        "    encoding='utf-8',                 # Ensure proper encoding\n",
        ")\n",
        "\n",
        "# Display the first few rows of the dataframe to inspect the data\n",
        "test_data = pd.read_csv(\n",
        "    \"persuade_corpus_2.0_train.csv\",\n",
        "    on_bad_lines=\"skip\",              # Skip rows with errors\n",
        "    quoting=csv.QUOTE_MINIMAL,        # Use minimal quoting (only when necessary)\n",
        "    quotechar='\"',                    # Handle quotes correctly (even though none expected)\n",
        "    engine='python',                  # Use the Python engine for better error handling\n",
        "    encoding='utf-8',                 # Ensure proper encoding\n",
        ")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "GGT_51WGuQUt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c811c6fe-8bf8-414d-80aa-66375146f694"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-19 13:54:18--  https://raw.githubusercontent.com/Azraelix316/AI_Club_Files/main/persuade_2.0_human_scores_demo_id_github.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75931833 (72M) [text/plain]\n",
            "Saving to: ‘persuade_corpus_2.0_train.csv’\n",
            "\n",
            "persuade_corpus_2.0 100%[===================>]  72.41M   273MB/s    in 0.3s    \n",
            "\n",
            "2025-02-19 13:54:24 (273 MB/s) - ‘persuade_corpus_2.0_train.csv’ saved [75931833/75931833]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we do some data cleaning and sorting. This is because we used a csv for our training data and is specifically because we used the Persuade Corpus.\n"
      ],
      "metadata": {
        "id": "aF0ySfq2Rr8D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsO1RL_UZDwd",
        "outputId": "9b5e061f-5a43-408d-ab95-65ffb0e5bc5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "# Convert scores to numeric and handle any errors\n",
        "train_data['holistic_essay_score'] = pd.to_numeric(train_data['holistic_essay_score'], errors='coerce')\n",
        "print(1)\n",
        "# Drop rows with NaN in 'holistic_essay_score'\n",
        "train_data = train_data.dropna(subset=['holistic_essay_score'])\n",
        "print(2)\n",
        "# Step 2: Prepare the training and testing datasets\n",
        "X_train = train_data['full_text']  # Text column in the training data\n",
        "y_train = train_data['holistic_essay_score']  # Score column in the training data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ACTUAL CODE"
      ],
      "metadata": {
        "id": "qqqJ3zPvS5Tc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything above is just to grab the actual data and is not a part of TFIDF. Now, we actually start using the algorithms. First, we create a tfidf vectorizer for each score, 1 through 6.\n",
        "\n",
        "Each vectorizer is used to grade how **similar** our test essay is to a **document**, each level of our training set. In this case, we create 6 vectorizers to test similarity between the test set and each level - basically, we check how similar our test data is to level 1 writing, level 2 writing, and so on."
      ],
      "metadata": {
        "id": "A8xZBCWbTV4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "one =[]\n",
        "two=[]\n",
        "three=[]\n",
        "four =[]\n",
        "five = []\n",
        "six = []\n",
        "scores = [one, two, three, four, five, six]\n",
        "\n",
        "for i in range(len(y_train)):\n",
        "  scores[y_train[i]-1].append(X_train[i]) # Text column in the test data\n",
        "\n",
        "# Initialize vectorizers for each score (1-6)\n",
        "vectorizers = [TfidfVectorizer(stop_words='english', lowercase=True) for _ in range(6)]\n",
        "\n",
        "# Fit and transform the training data to get the TF-IDF matrix for each score group\n",
        "tfidf_matrices = [vectorizers[i].fit_transform(scores[i]) for i in range(6)]"
      ],
      "metadata": {
        "id": "KzNl-pgLJxeI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we initialize our X_test field, which is the actual essay we are using. This is a horrendous filler essay, replace it with any text you want to use."
      ],
      "metadata": {
        "id": "ktBqwMYVS8kX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pd.Series([\"So like, let’s talk about stuff. Y’know, like, stuff that happens, stuff that we see, stuff that we just do without even thinking, like, all that. I was thinking about it the other day, like, there’s this thing where we all do stuff without even thinking and then we just move on with life like it’s no biggie. But honestly, sometimes it’s a big deal and sometimes it’s just whatever. Like, you ever think about stuff you did last week? I bet you probably forgot. But it was there, you know? Stuff just goes and then we forget, but it’s always around us. Kinda like how we eat, but we don’t really care what we eat until we eat it and then we’re like, “oh yeah, that was food.” But then we’re full and move on, and that’s the vibe. Also, I’ve been seeing some stuff happening on social media, and it’s like, whoa, there’s too much stuff happening at once. Everyone’s just out here posting stuff that nobody really cares about, but they think people care, so it’s a whole thing. Like, who even asked for all this stuff? It’s just too much stuff all the time, and it’s hard to keep up. Then, there’s the whole “chill” thing. We’re supposed to chill, right? But no one knows how to chill anymore. We just keep getting caught up in the stuff and forget how to chill. People used to just sit and talk or do nothing, but now we gotta be “doing stuff” all the time. Like, why is everything so fast-paced? Just slow down, bro. And then, like, let’s talk about school or work or whatever you do, ‘cause it’s all the same. It’s just more stuff. You get through one thing, and there’s another thing right after it, and you gotta get through that too. It’s like one long line of stuff, and there’s no end to it. So much stuff. And nobody even knows why we’re doing any of it. It’s just stuff, and we do it ‘cause that’s what we do. No biggie, right? In conclusion, life is just a whole lot of stuff. Stuff everywhere, stuff in your face, stuff that never stops. So just remember, when you’re doing all the stuff, just don’t forget to chill and breathe ‘cause at the end of the day, it’s all just stuff. Yeah \"], dtype=\"object\")  # Create a new Series with the desired element and dtype\n",
        "# X_test = test_data['full_text']\n",
        "# Transform the test data using the same vectorizer for each score group\n",
        "X_test_tfidf = [vectorizers[i].transform(X_test) for i in range(6)]"
      ],
      "metadata": {
        "id": "6ndSw-KhS71X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we create a prediction by summing up every tfidf score for each vectorizer, 1-6 and after some normalization, we create a final output prediction."
      ],
      "metadata": {
        "id": "n_Pb-yDdSfdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now for prediction (find which vectorizer has the highest sum for each test document)\n",
        "res = []\n",
        "\n",
        "# Iterate over rows (documents) in the test data\n",
        "for i in range(X_test_tfidf[0].shape[0]):  # Assume all matrices have the same number of rows\n",
        "    column_sums = [np.array(matrix[i, :].sum()) for matrix in X_test_tfidf]\n",
        "\n",
        "    # Find the index of the matrix with the maximum sum\n",
        "    max_matrix_index = max(range(6), key=lambda x: column_sums[x]/len(scores[x]))  # 0-indexed max\n",
        "    res.append(max_matrix_index + 1)  # Convert to 1-indexed score\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZxLrJwxBtUQ",
        "outputId": "b2f0fe6c-0808-46fd-914a-9930d6adf6e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}